<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning Fundamentals-1 | HackThe4O4</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="title: Machine Learning Fundamentals-1 mathjax: trueWhat Is Machine Learning?Imagine you have lots of data points — for example:     Study Hours (x) Test Score (y)     1 40   2 55   3 65   4 80     Y">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Fundamentals-1">
<meta property="og:url" content="https://no-flag.com/2025/10/18/ML-Basic1/index.html">
<meta property="og:site_name" content="HackThe4O4">
<meta property="og:description" content="title: Machine Learning Fundamentals-1 mathjax: trueWhat Is Machine Learning?Imagine you have lots of data points — for example:     Study Hours (x) Test Score (y)     1 40   2 55   3 65   4 80     Y">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-10-17T21:38:59.785Z">
<meta property="article:modified_time" content="2025-10-27T08:52:43.176Z">
<meta property="article:author" content="Noflag">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="HackThe4O4" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/cat.jpg">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  


<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/mela.gif" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>

		

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/2024/12/21/hello-world/">About Me</a></li>
				        
							<li><a href="/archives">All articles</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/mela.gif" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/2024/12/21/hello-world/">About Me</a></li>
		        
					<li><a href="/archives">All articles</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-ML-Basic1" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/10/18/ML-Basic1/" class="article-date">
  	<time datetime="2025-10-17T21:38:59.785Z" itemprop="datePublished">2025-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Machine Learning Fundamentals-1
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        

        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title: Machine Learning Fundamentals-1</p>
<h2 id="mathjax-true"><a href="#mathjax-true" class="headerlink" title="mathjax: true"></a>mathjax: true</h2><h2 id="What-Is-Machine-Learning"><a href="#What-Is-Machine-Learning" class="headerlink" title="What Is Machine Learning?"></a>What Is Machine Learning?</h2><p>Imagine you have lots of data points — for example:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Study Hours (x)</th>
<th>Test Score (y)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>40</td>
</tr>
<tr>
<td>2</td>
<td>55</td>
</tr>
<tr>
<td>3</td>
<td>65</td>
</tr>
<tr>
<td>4</td>
<td>80</td>
</tr>
</tbody>
</table>
</div>
<p>You’d like a machine to find a rule that converts <code>x</code> (study hours) → <code>y</code> (test score).</p>
<p>That rule is a <strong>function</strong>:</p>
<script type="math/tex; mode=display">
y = f(x)</script><p>Machine learning is just a <strong>function-guessing game</strong>:</p>
<ol>
<li>Guess a function form (e.g., straight line <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="10.776ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 4763 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1823.6,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(2539.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3333.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4334,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>).</li>
<li>Measure how wrong it is.</li>
<li>Adjust the numbers (parameters <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>) to make predictions closer to reality.</li>
</ol>
<p><strong>Real-world analogy:</strong><br>It’s like adjusting the temperature knob on a shower — you keep tweaking until the water feels just right.</p>
<p><strong>💡 Example:</strong></p>
<ul>
<li>A spam filter guesses a function that maps <em>email text → spam probability</em>.</li>
<li>A speech recognizer guesses <em>sound wave → word</em>.</li>
<li>A recommender system guesses <em>user history → movie score</em>.</li>
</ul>
<hr>
<h2 id="Linear-vs-Nonlinear-Models"><a href="#Linear-vs-Nonlinear-Models" class="headerlink" title="Linear vs Nonlinear Models"></a>Linear vs Nonlinear Models</h2><h3 id="🔹-Linear-Models-—-Simple-but-Limited"><a href="#🔹-Linear-Models-—-Simple-but-Limited" class="headerlink" title="🔹 Linear Models — Simple but Limited"></a>🔹 Linear Models — Simple but Limited</h3><p>Form:</p>
<script type="math/tex; mode=display">
y = wx + b</script><p>Visual: a <strong>straight line</strong> fitting the data.</p>
<p><strong>Analogy:</strong><br>A ruler — perfect for straight edges, terrible for curves.</p>
<p><strong>When it works:</strong><br>If your data roughly lies in a straight trend (e.g., more study → higher score).</p>
<p><strong>When it fails:</strong><br>If your data curves (like house prices vs size that first rise fast then plateau).</p>
<hr>
<h3 id="🔹-Nonlinear-Models-—-Curves-amp-Flexibility"><a href="#🔹-Nonlinear-Models-—-Curves-amp-Flexibility" class="headerlink" title="🔹 Nonlinear Models — Curves & Flexibility"></a>🔹 Nonlinear Models — Curves &amp; Flexibility</h3><p>We can add nonlinear terms:</p>
<script type="math/tex; mode=display">
y = w_1 x + w_2 x^2 + b</script><p>or use neural networks that <em>learn their own</em> nonlinearities.</p>
<p><strong>Analogy:</strong><br>Instead of one ruler, now you have <strong>flexible rubber bands</strong> — you can bend them to match any shape.</p>
<p><strong>Visual intuition:</strong><br>Linear: ————-<br>Nonlinear: <del>~</del>~~~</p>
<p><strong>Why we need nonlinearity:</strong><br>Without it, stacking more linear layers still gives you… another straight line.<br>Adding nonlinear “bends” (activation functions) gives networks expressive power — this is what lets them model speech, images, and text.</p>
<hr>
<h2 id="The-Neural-Network-—-Thinking-Like-LEGO-Blocks"><a href="#The-Neural-Network-—-Thinking-Like-LEGO-Blocks" class="headerlink" title="The Neural Network — Thinking Like LEGO Blocks"></a>The Neural Network — Thinking Like LEGO Blocks</h2><p>A neural network is a big chain of <strong>tiny functions</strong> stacked together.</p>
<p>Each tiny function (a “neuron”) does:</p>
<script type="math/tex; mode=display">
z = w \cdot x + b</script><script type="math/tex; mode=display">
a = g(z)</script><p>where <code>g</code> is an <strong>activation function</strong> (like Sigmoid, ReLU, etc.).</p>
<h3 id="🧩-Layers-as-LEGO-Bricks"><a href="#🧩-Layers-as-LEGO-Bricks" class="headerlink" title="🧩 Layers as LEGO Bricks"></a>🧩 Layers as LEGO Bricks</h3><ul>
<li><strong>Input layer:</strong> feeds in features (like study hours, age, weather, etc.)</li>
<li><strong>Hidden layers:</strong> each transforms inputs a little.</li>
<li><strong>Output layer:</strong> produces final prediction (e.g., “spam or not spam”).</li>
</ul>
<p><strong>Analogy:</strong><br>Each layer transforms the input just like filters on Instagram — each filter adds its own twist until you get the final photo style.</p>
<hr>
<h3 id="💡-Example-Predict-House-Price"><a href="#💡-Example-Predict-House-Price" class="headerlink" title="💡 Example: Predict House Price"></a>💡 Example: Predict House Price</h3><ul>
<li>Input: [size, location, number of rooms]  </li>
<li><p>Hidden layer neurons each learn patterns like:</p>
<ul>
<li>Neuron 1 → “large house in good area”  </li>
<li>Neuron 2 → “too many rooms, old area”  </li>
<li>Output → weighted combination → final price.</li>
</ul>
</li>
</ul>
<p>That’s literally what “deep” means in <strong>deep learning</strong> — multiple layers of abstraction.</p>
<hr>
<h2 id="Loss-Function-—-How-the-Model-Measures-Mistakes"><a href="#Loss-Function-—-How-the-Model-Measures-Mistakes" class="headerlink" title="Loss Function — How the Model Measures Mistakes"></a>Loss Function — How the Model Measures Mistakes</h2><p>Loss = “how wrong we are.”</p>
<h3 id="🎯-Example"><a href="#🎯-Example" class="headerlink" title="🎯 Example"></a>🎯 Example</h3><p>You predict test scores:<br>True = 90, Predicted = 70 → Error = 20.</p>
<p>If we square it → 400 (we penalize big mistakes more).</p>
<h3 id="🧮-Common-Losses"><a href="#🧮-Common-Losses" class="headerlink" title="🧮 Common Losses"></a>🧮 Common Losses</h3><h4 id="Mean-Squared-Error-MSE"><a href="#Mean-Squared-Error-MSE" class="headerlink" title="Mean Squared Error (MSE)"></a>Mean Squared Error (MSE)</h4><script type="math/tex; mode=display">
L = \frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2</script><p>Good for <strong>regression</strong> (predicting numbers).</p>
<h4 id="Mean-Absolute-Error-MAE"><a href="#Mean-Absolute-Error-MAE" class="headerlink" title="Mean Absolute Error (MAE)"></a>Mean Absolute Error (MAE)</h4><script type="math/tex; mode=display">
L = \frac{1}{N}\sum_{i=1}^{N} \lvert y_i - \hat{y}_i \rvert</script><p>Less sensitive to outliers.</p>
<p><strong>Analogy:</strong><br>Loss is like a “distance meter” — it tells the model how far its guess is from the truth.</p>
<hr>
<h3 id="💡-Intuitive-Example"><a href="#💡-Intuitive-Example" class="headerlink" title="💡 Intuitive Example:"></a>💡 Intuitive Example:</h3><p>If your model is predicting pizza delivery times:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>True (min)</th>
<th>Predicted (min)</th>
<th>Error</th>
<th>MSE</th>
</tr>
</thead>
<tbody>
<tr>
<td>30</td>
<td>35</td>
<td>+5</td>
<td>25</td>
</tr>
<tr>
<td>40</td>
<td>39</td>
<td>-1</td>
<td>1</td>
</tr>
<tr>
<td>50</td>
<td>70</td>
<td>+20</td>
<td>400</td>
</tr>
</tbody>
</table>
</div>
<p>The <strong>total MSE</strong> will be high because of that one 20-minute late pizza — the model learns to avoid such big mistakes.</p>
<hr>
<h2 id="Optimization-—-How-the-Model-Learns"><a href="#Optimization-—-How-the-Model-Learns" class="headerlink" title="Optimization — How the Model Learns"></a>Optimization — How the Model Learns</h2><p>Goal: find parameters (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>) that minimize loss.</p>
<hr>
<h3 id="⚙️-Gradient-Direction-of-Steepest-Ascent"><a href="#⚙️-Gradient-Direction-of-Steepest-Ascent" class="headerlink" title="⚙️ Gradient = Direction of Steepest Ascent"></a>⚙️ Gradient = Direction of Steepest Ascent</h3><p>But we want to go <strong>downhill</strong>, so we move in the opposite direction.</p>
<script type="math/tex; mode=display">
w_{\text{new}} = w - \eta \frac{\partial L}{\partial w}</script><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.124ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 497 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> (eta) is the <strong>learning rate</strong>.</p>
<p><strong>Analogy:</strong><br>You’re hiking blindfolded in a foggy valley. The gradient tells you “the slope is downward that way.” You take a small step down — that’s gradient descent.</p>
<hr>
<h3 id="💡-Example"><a href="#💡-Example" class="headerlink" title="💡 Example:"></a>💡 Example:</h3><p>Imagine loss = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 1152.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>.<br>The slope = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.751ex" height="1.532ex" role="img" focusable="false" viewBox="0 -666 1216 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>.<br>If <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.899ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 3049.6 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(993.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2049.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container>, slope = 20. You step down by <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="5.021ex" height="1.995ex" role="img" focusable="false" viewBox="0 -666 2219.4 882"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(719.2,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mn" transform="translate(1219.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container>.<br>After several steps, you reach <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.768ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2549.6 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(993.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2049.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container> (the minimum).</p>
<hr>
<h2 id="Forward-amp-Backward-Propagation"><a href="#Forward-amp-Backward-Propagation" class="headerlink" title="Forward & Backward Propagation"></a>Forward &amp; Backward Propagation</h2><h3 id="Forward-pass"><a href="#Forward-pass" class="headerlink" title="Forward pass:"></a>Forward pass:</h3><p>Compute output from inputs:</p>
<script type="math/tex; mode=display">
y_{\text{pred}} = f(x; \theta)</script><p>and loss <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="9.828ex" height="2.347ex" role="img" focusable="false" viewBox="0 -750 4344.1 1037.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1560,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2004.7,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(556,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(948,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(1392,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(3955.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>.</p>
<h3 id="⬅️-Backward-pass"><a href="#⬅️-Backward-pass" class="headerlink" title="⬅️ Backward pass:"></a>⬅️ Backward pass:</h3><p>Compute how much each parameter contributed to the loss — using the <strong>chain rule</strong> of calculus.</p>
<p><strong>Analogy:</strong><br>Think of cooking: if the final dish is too salty, backprop tells you <em>which ingredient</em> (salt, sauce, etc.) caused it and how much to adjust next time.</p>
<p><strong>Mathematical core:</strong><br>It’s just repeatedly applying</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}</script><p>through each layer.</p>
<hr>
<h2 id="Code-Examples-—-See-It-in-Action"><a href="#Code-Examples-—-See-It-in-Action" class="headerlink" title="Code Examples — See It in Action"></a>Code Examples — See It in Action</h2><h3 id="A-Linear-Regression-Closed-Form-Solution"><a href="#A-Linear-Regression-Closed-Form-Solution" class="headerlink" title="(A) Linear Regression (Closed-Form Solution)"></a>(A) Linear Regression (Closed-Form Solution)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate simple data</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">2.0</span> * x + <span class="number">1.0</span> + np.random.normal(<span class="number">0</span>, <span class="number">1.0</span>, size=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare matrix [x, 1]</span></span><br><span class="line">X = np.vstack([x, np.ones_like(x)]).T</span><br><span class="line">theta = np.linalg.inv(X.T @ X) @ X.T @ y</span><br><span class="line">w, b = theta</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Closed-form solution:"</span>, w, b)</span><br></pre></td></tr></table></figure>
<h3 id="B-Linear-Regression-Gradient-Descent"><a href="#B-Linear-Regression-Gradient-Descent" class="headerlink" title="(B) Linear Regression (Gradient Descent)"></a>(B) Linear Regression (Gradient Descent)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">50</span>)</span><br><span class="line">y = <span class="number">3.5</span> * x - <span class="number">0.5</span> + np.random.normal(<span class="number">0</span>, <span class="number">1.0</span>, size=x.shape)</span><br><span class="line"></span><br><span class="line">w, b = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">epochs = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    y_pred = w * x + b</span><br><span class="line">    loss = np.mean((y - y_pred)**<span class="number">2</span>)</span><br><span class="line">    dw = (-<span class="number">2</span>/<span class="built_in">len</span>(x)) * np.<span class="built_in">sum</span>(x * (y - y_pred))</span><br><span class="line">    db = (-<span class="number">2</span>/<span class="built_in">len</span>(x)) * np.<span class="built_in">sum</span>(y - y_pred)</span><br><span class="line">    w -= lr * dw</span><br><span class="line">    b -= lr * db</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch}</span>, Loss <span class="subst">{loss:<span class="number">.4</span>f}</span>, w <span class="subst">{w:<span class="number">.2</span>f}</span>, b <span class="subst">{b:<span class="number">.2</span>f}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>Watch</strong>: The line slowly adjusts slope &amp; intercept until it fits all points.</p>
<h3 id="C-One-Neuron-Network-with-Sigmoid"><a href="#C-One-Neuron-Network-with-Sigmoid" class="headerlink" title="(C) One-Neuron Network with Sigmoid"></a>(C) One-Neuron Network with Sigmoid</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">0.5</span>, -<span class="number">1.2</span>],</span><br><span class="line">              [<span class="number">1.0</span>,  <span class="number">0.2</span>],</span><br><span class="line">              [-<span class="number">0.3</span>, <span class="number">0.8</span>]])</span><br><span class="line">y = np.array([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line">W = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)*<span class="number">0.1</span></span><br><span class="line">b = np.zeros((<span class="number">1</span>,))</span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):</span><br><span class="line">    z = X @ W + b</span><br><span class="line">    a = sigmoid(z)</span><br><span class="line">    loss = np.mean((a - y)**<span class="number">2</span>)</span><br><span class="line">    dloss_dz = <span class="number">2</span>*(a - y) * a*(<span class="number">1</span> - a)</span><br><span class="line">    dW = X.T @ dloss_dz / <span class="built_in">len</span>(X)</span><br><span class="line">    db = np.<span class="built_in">sum</span>(dloss_dz) / <span class="built_in">len</span>(X)</span><br><span class="line">    W -= lr * dW</span><br><span class="line">    b -= lr * db</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(step, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>Explanation:</strong><br>This is the smallest possible neural network — it learns to map 2-D inputs to 0/1 outputs.</p>
<hr>
<h2 id="Common-Pitfalls-amp-Practical-Tips"><a href="#Common-Pitfalls-amp-Practical-Tips" class="headerlink" title="Common Pitfalls & Practical Tips"></a>Common Pitfalls &amp; Practical Tips</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Problem</th>
<th>Why It Happens</th>
<th>Fix</th>
</tr>
</thead>
<tbody>
<tr>
<td>🚫 Doesn’t converge</td>
<td>Learning rate too high</td>
<td>Lower it (1e-3 → 1e-4)</td>
</tr>
<tr>
<td>🐢 Too slow</td>
<td>Learning rate too low</td>
<td>Increase it slightly</td>
</tr>
<tr>
<td>😵 Exploding gradients</td>
<td>Network too deep / bad initialization</td>
<td>Normalize inputs, use ReLU, use Adam optimizer</td>
</tr>
<tr>
<td>😴 Underfitting</td>
<td>Model too simple</td>
<td>Add layers or features</td>
</tr>
<tr>
<td>😈 Overfitting</td>
<td>Model too complex</td>
<td>Add dropout / regularization / more data</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="💡-Example-Visualizing-Overfitting"><a href="#💡-Example-Visualizing-Overfitting" class="headerlink" title="💡 Example: Visualizing Overfitting"></a>💡 Example: Visualizing Overfitting</h3><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Train Loss ↓↓↓↓↓↓</span><br><span class="line">Test  Loss ↓↓↓↓↑↑↑</span><br></pre></td></tr></table></figure>
<p>When test loss starts increasing, the model memorizes noise instead of learning real patterns.</p>
<hr>
<h3 id="🔧-Tuning-Tips"><a href="#🔧-Tuning-Tips" class="headerlink" title="🔧 Tuning Tips"></a>🔧 Tuning Tips</h3><ul>
<li>Always normalize input data (mean=0, std=1).</li>
<li>Start with small models first, then grow.</li>
<li>Plot training &amp; validation loss — if they diverge → overfitting.</li>
<li>Use optimizers like <strong>Adam</strong> instead of plain SGD for stability.</li>
</ul>
<hr>
<h2 id="Glossary-—-Plain-ML-Terms"><a href="#Glossary-—-Plain-ML-Terms" class="headerlink" title="Glossary — Plain ML Terms"></a>Glossary — Plain ML Terms</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Feature</strong></td>
<td>Input variable (like “hours studied”)</td>
</tr>
<tr>
<td><strong>Label / Target</strong></td>
<td>Output we want to predict (like “score”)</td>
</tr>
<tr>
<td><strong>Model</strong></td>
<td>A mathematical function that maps features → predictions</td>
</tr>
<tr>
<td><strong>Parameter (w, b)</strong></td>
<td>Adjustable knobs the model learns</td>
</tr>
<tr>
<td><strong>Loss Function</strong></td>
<td>Measures how wrong predictions are</td>
</tr>
<tr>
<td><strong>Gradient</strong></td>
<td>Direction telling how to change parameters</td>
</tr>
<tr>
<td><strong>Learning Rate</strong></td>
<td>Step size for updates</td>
</tr>
<tr>
<td><strong>Epoch</strong></td>
<td>One full pass through the training data</td>
</tr>
<tr>
<td><strong>Activation Function</strong></td>
<td>Nonlinear bend added after linear combination</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>When model memorizes instead of generalizing</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="Summary-—-Big-Picture"><a href="#Summary-—-Big-Picture" class="headerlink" title="Summary — Big Picture"></a>Summary — Big Picture</h2><p>Machine Learning is the art of:</p>
<ol>
<li><strong>Guessing a function</strong> that maps inputs to outputs.</li>
<li><strong>Measuring errors</strong> using a loss function.</li>
<li><strong>Adjusting parameters</strong> with gradient descent.</li>
<li><strong>Repeating</strong> until predictions are good enough.</li>
</ol>
<p>Neural networks do the same — but stack many layers and nonlinearities, allowing them to approximate almost any function in the world.</p>
<blockquote>
<p>“Machine Learning isn’t magic — it’s just math with feedback.”</p>
</blockquote>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/10/27/Communication/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          The Art of Communication From Talking to Being Trusted
        
      </div>
    </a>
  
  
    <a href="/2025/10/17/LLM_Security/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">The OWASP Inspired LLM Security Playbook Risks, Real World Cases, and Defenses</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>





</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2025 Noflag
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>